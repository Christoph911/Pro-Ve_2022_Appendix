{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df7af52c-f4ce-4223-be89-fc10a007059f",
   "metadata": {},
   "source": [
    "# Retriever Model and Pipeline Evaluation on LegalQuAD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbfe859-c6bf-4fb7-9321-904bc5e8754b",
   "metadata": {},
   "source": [
    "This notebook contains the Evaluation of different Retriever and Reader Models on the German LegalQuAD test dataset. \n",
    "The experiments were conducted as part of the Paper \"Question Answering in German Caselaw\" published at the Pro-Ve 2022.\n",
    "A detailed description of the experiments can be found in the paper. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afd9ad9-16fd-4905-9c50-35a6e8e5c3e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Preprequisits: \n",
    "\n",
    "- Python >= 3.7.5\n",
    "- farm-haystack >= 1.0\n",
    "- PyTorch\n",
    "- Running ElasticSearch DocumentStore (self-hosted or managed)\n",
    "- GPU support is highly recommended"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6882560e-bfc6-448b-ae47-af33a6697213",
   "metadata": {},
   "source": [
    "## Table of Contents:\n",
    "\n",
    "* [1. Database setup](#chapter1)\n",
    "* [2. Data Preprocessing & Database indexing](#chapter2)\n",
    "* [3. Retriever and Reader model initialisation](#chapter3)\n",
    "* [4. Evaluation & Results](#chapter4) \n",
    "    * [4.1 Retriever models standalone](#section_4_1) \n",
    "        * [4.1.1 Retriever model on top_k=10](#section_4_1_1)\n",
    "        * [4.1.2 Retriever model with top_k=range(10,110,10)](#section_4_1_2)\n",
    "    * [4.2 QA-Pipeline](#section_4_2) \n",
    "        * [4.2.1 QA-Pipeline with BM25](#section_4_2_1)\n",
    "        * [4.2.2 QA-Pipeline with MFAQ](#section_4_2_2)\n",
    "        * [4.2.3 QA Pipeline with Ensemble Retriever (BM25 + MFAQ)](#section_4_2_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa9989e-fcf9-4f7a-a82a-4c911e99c16a",
   "metadata": {},
   "source": [
    "## 1. Database setup <a class=\"anchor\" id=\"chapter1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f51f380-d6d4-4792-8f0a-3f506f9ba93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ElasticSearch DocumentStore\n",
    "from haystack.document_stores import ElasticsearchDocumentStore\n",
    "\n",
    "# Define doc and label index for the database\n",
    "doc_index = \"eval_docs\"\n",
    "label_index = \"eval_labels\"\n",
    "\n",
    "# Init and connect to ElasticSearch\n",
    "document_store = ElasticsearchDocumentStore(host=\"d959cba9e11f9a.lhrtunnel.link\",\n",
    "                                            port=443,\n",
    "                                            scheme='https',\n",
    "                                            username=\"\",\n",
    "                                            password=\"\",\n",
    "                                            index=doc_index,\n",
    "                                            label_index=label_index,\n",
    "                                            embedding_field=\"emb\",\n",
    "                                            embedding_dim=768,\n",
    "                                            excluded_meta_data=[\"emb\"],\n",
    "                                            similarity=\"dot_product\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4a8502-8717-4e45-bdce-a7a694384ffe",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Data Preprocessing & Database indexing <a class=\"anchor\" id=\"chapter2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1404a4-6e5a-4202-974a-b81ae775b000",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import Preprocessor\n",
    "from haystack.nodes import PreProcessor\n",
    "\n",
    "# Init preprocessor\n",
    "# Split documents after 200 words\n",
    "preprocessor = PreProcessor(\n",
    "    language='de',\n",
    "    split_length=200,\n",
    "    split_overlap=0,\n",
    "    split_respect_sentence_boundary=False,\n",
    "    clean_empty_lines=False,\n",
    "    clean_whitespace=False\n",
    ")\n",
    "\n",
    "# delete indicies to make sure, there are no duplicates\n",
    "document_store.delete_documents(index=doc_index)\n",
    "document_store.delete_documents(index=label_index)\n",
    "\n",
    "# Convert SQuAD dataset into haystack document format\n",
    "document_store.add_eval_data(\n",
    "    filename=\"../data/legal_squad_test.json\",\n",
    "    doc_index=doc_index,\n",
    "    label_index=label_index,\n",
    "    preprocessor=preprocessor\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae703197-6198-4077-86aa-dcd4a4ffbd7a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Retriever and Reader model initialisation <a class=\"anchor\" id=\"chapter3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7990ffa-1109-424a-b845-df888b9b922d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import Retrieval methods \n",
    "from haystack.nodes import TfidfRetriever, ElasticsearchRetriever, DensePassageRetriever, EmbeddingRetriever\n",
    "\n",
    "# Init TF-IDF\n",
    "retriever_tfidf = TfidfRetriever(document_store=document_store)\n",
    "# Init BM25\n",
    "retriever_bm25 = ElasticsearchRetriever(document_store=document_store)\n",
    "# Init EmbeddingRetriver from huggingface\n",
    "retriever_emb = EmbeddingRetriever(document_store=document_store, embedding_model=\"clips/mfaq\")\n",
    "# Update passage embeddings inside the document store\n",
    "document_store.update_embeddings(retriever_emb, index=doc_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74766ad5-9e0f-4603-9ac3-c0164ba37117",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Evaluation & Results <a class=\"anchor\" id=\"chapter4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0897bb4f-31fa-4158-9ccb-cc34b9569545",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import Pipeline and pre-defined pipelines\n",
    "from haystack import Pipeline\n",
    "from haystack.pipelines import ExtractiveQAPipeline, DocumentSearchPipeline, JoinDocuments\n",
    "# Import Evaluation results and Labels\n",
    "from haystack.schema import EvaluationResult, MultiLabel\n",
    "# set evaluation labels\n",
    "eval_labels = document_store.get_all_labels_aggregated(drop_negative_labels=True, drop_no_answers=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5775373a-33e1-4c3d-997f-47722c83ee57",
   "metadata": {},
   "source": [
    "### 4.1 Retriever models standalone <a class=\"anchor\" id=\"section_4_1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b97906-ea24-41a8-bd9a-fae779e522c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 4.1.1 Retriever model on top_k=10 <a class=\"anchor\" id=\"section_4_1_1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000c2aa0-fe90-456f-aec6-e153f6ac4192",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate all retriever models on top_k=10\n",
    "\n",
    "\n",
    "def get_retrieval_results(top_k=10):\n",
    "    \"\"\"\n",
    "    Method iterates over the defined retrieval methods\n",
    "    and evaluates their passage search capabilities. \n",
    "    \"\"\"\n",
    "    \n",
    "    # list with the retrieval methods initialized in chapter 3.  \n",
    "    retrieval_methods = [retriever_tfidf, retriever_bm25, retriever_emb]\n",
    "\n",
    "    for method in retrieval_methods:\n",
    "        # init document search pipeline\n",
    "        pipeline_ds = DocumentSearchPipeline(retriever=method)\n",
    "\n",
    "        # init evaluation pipeline\n",
    "        eval_result_pipeline = pipeline_ds.eval(\n",
    "            labels=eval_labels,\n",
    "            params={\"Retriever\": {\"top_k\": top_k}}\n",
    "        )\n",
    "\n",
    "        # calculate and print metrics\n",
    "        metrics = eval_result_pipeline.calculate_metrics()\n",
    "        print(f\"*** RETRIEVER RESULTS: {method.__str__()} ***\")\n",
    "        print(f'Retriever - Recall (single relevant document): {metrics[\"Retriever\"][\"recall_single_hit\"]}')\n",
    "        print(f'Retriever - Recall (multiple relevant documents): {metrics[\"Retriever\"][\"recall_multi_hit\"]}')\n",
    "        print(f'Retriever - Mean Reciprocal Rank: {metrics[\"Retriever\"][\"mrr\"]}')\n",
    "        print(f'Retriever - Precision: {metrics[\"Retriever\"][\"precision\"]}')\n",
    "        print(f'Retriever - Mean Average Precision: {metrics[\"Retriever\"][\"map\"]}')\n",
    "        print(\"******************************************************************\")\n",
    "\n",
    "# call method\n",
    "get_retrieval_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a173b970-6b6d-4ee5-8084-82a0d2ac5924",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 4.1.2 Retriever model with top_k=range(10,110,10) <a class=\"anchor\" id=\"section_4_1_2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d0fe06-68e0-4424-ac91-9e7399b62f4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate retriever models in range(10,110,10)\n",
    "\n",
    "def get_retrieval_results_in_range():\n",
    "    \"\"\"\n",
    "    Method iterates over defined retrieval methods\n",
    "    and evaluates their passage search capabilities. \n",
    "    \"\"\"\n",
    "    \n",
    "    # list with the retrieval methods  initialized in chapter 3.  \n",
    "    retrieval_methods = [retriever_tfidf, retriever_bm25, retriever_emb]\n",
    "    \n",
    "    for top_k in range (10,110,10):\n",
    "        print(f\"*** Results on top_k: {top_k} ***\")\n",
    "        \n",
    "        for method in retrieval_methods:\n",
    "            # init document search pipeline\n",
    "            pipeline_ds = DocumentSearchPipeline(retriever=method)\n",
    "\n",
    "            # init evaluation pipeline\n",
    "            eval_result_pipeline = pipeline_ds.eval(\n",
    "                labels=eval_labels,\n",
    "                params={\"Retriever\": {\"top_k\": top_k}}\n",
    "            )\n",
    "\n",
    "            # calculate and print metrics\n",
    "            metrics = eval_result_pipeline.calculate_metrics()\n",
    "            print(f\"*** RETRIEVER RESULTS: {method.__str__()} ***\")\n",
    "            print(f'Retriever - Recall (single relevant document): {metrics[\"Retriever\"][\"recall_single_hit\"]}')\n",
    "            print(f'Retriever - Recall (multiple relevant documents): {metrics[\"Retriever\"][\"recall_multi_hit\"]}')\n",
    "            print(f'Retriever - Mean Reciprocal Rank: {metrics[\"Retriever\"][\"mrr\"]}')\n",
    "            print(f'Retriever - Precision: {metrics[\"Retriever\"][\"precision\"]}')\n",
    "            print(f'Retriever - Mean Average Precision: {metrics[\"Retriever\"][\"map\"]}')\n",
    "            print(\"******************************************************************\")\n",
    "\n",
    "# call method\n",
    "get_retrieval_results_in_range()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6470301e-4b98-47fb-954f-4470f37e1f36",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4.2 QA-Pipeline <a class=\"anchor\" id=\"section_4_2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0935c25-b12c-4346-8d3c-d4f196e4327e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import FARMReader\n",
    "from haystack.nodes.reader import FARMReader\n",
    "\n",
    "# Init fine_tuned reader\n",
    "reader_fine_tuned = FARMReader(\"finetuned_models/GELECTRA-large-LegalQuAD-new\", return_no_answer=True)\n",
    "\n",
    "# Init base reader\n",
    "reader_base = FARMReader(\"deepset/gelectra-base-germanquad\", return_no_answer=True)\n",
    "\n",
    "# Init large reader\n",
    "reader_large = FARMReader(\"deepset/gelectra-large-germanquad\", return_no_answer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bd3583-7535-4c8e-bd2c-9169c80ae2a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 4.2.1 QA-Pipeline with BM25 <a class=\"anchor\" id=\"section_4_2_1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44df2723-3bb5-4b90-9246-1c438d885b3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fine-tuned reader evaluation\n",
    "pipeline_qa_fine_tuned = ExtractiveQAPipeline(reader=reader_fine_tuned, retriever=retriever_bm25)\n",
    "\n",
    "eval_result_pipeline = pipeline_qa_fine_tuned.eval(\n",
    "            labels=eval_labels,\n",
    "            params={\"Retriever\": {\"top_k\": 10}, \"Reader\": {\"top_k\": 5}}\n",
    "        )\n",
    "\n",
    "# calculate and print metrics\n",
    "metrics = eval_result_pipeline.calculate_metrics()\n",
    "print(f\"*** RESULTS: Reader Fine-tuned ***\")\n",
    "print(f'Reader - F1-Score: {metrics[\"Reader\"][\"f1\"]}')\n",
    "print(f'Reader - Exact Match: {metrics[\"Reader\"][\"exact_match\"]}')\n",
    "print(\"******************************************************************\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa80e0f-5f02-4c57-80db-ab290ad794cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Base reader evaluation\n",
    "pipeline_qa_base = ExtractiveQAPipeline(reader=reader_base, retriever=retriever_bm25)\n",
    "\n",
    "eval_result_pipeline = pipeline_qa_base.eval(\n",
    "            labels=eval_labels,\n",
    "            params={\"Retriever\": {\"top_k\": 10}, \"Reader\": {\"top_k\": 5}}\n",
    "        )\n",
    "\n",
    "# calculate and print metrics\n",
    "metrics = eval_result_pipeline.calculate_metrics()\n",
    "print(f\"*** RESULTS: Reader BASE ***\")\n",
    "print(f'Reader - F1-Score: {metrics[\"Reader\"][\"f1\"]}')\n",
    "print(f'Reader - Exact Match: {metrics[\"Reader\"][\"exact_match\"]}')\n",
    "print(\"******************************************************************\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae106aca-9d0e-4ae4-86e1-d98dffd72c31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Large reader evaluation\n",
    "pipeline_qa_large = ExtractiveQAPipeline(reader=reader_large, retriever=retriever_bm25)\n",
    "\n",
    "eval_result_pipeline = pipeline_qa_large.eval(\n",
    "            labels=eval_labels,\n",
    "            params={\"Retriever\": {\"top_k\": 10}, \"Reader\": {\"top_k\": 5}}\n",
    "        )\n",
    "\n",
    "# calculate and print metrics\n",
    "metrics = eval_result_pipeline.calculate_metrics()\n",
    "print(f\"*** RESULTS: Reader LARGE ***\")\n",
    "print(f'Reader - F1-Score: {metrics[\"Reader\"][\"f1\"]}')\n",
    "print(f'Reader - Exact Match: {metrics[\"Reader\"][\"exact_match\"]}')\n",
    "print(\"******************************************************************\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4580e6-d2d8-4b1a-925f-42151c71b067",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 4.2.2 QA-Pipeline with MFAQ <a class=\"anchor\" id=\"section_4_2_2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b8a2bf-0f83-425a-87e1-898daaf4e769",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fine-tuned reader evaluation\n",
    "pipeline_qa_fine_tuned = ExtractiveQAPipeline(reader=reader_fine_tuned, retriever=retriever_emb)\n",
    "\n",
    "eval_result_pipeline = pipeline_qa_fine_tuned.eval(\n",
    "            labels=eval_labels,\n",
    "            params={\"Retriever\": {\"top_k\": 10}, \"Reader\": {\"top_k\": 5}}\n",
    "        )\n",
    "\n",
    "# calculate and print metrics\n",
    "metrics = eval_result_pipeline.calculate_metrics()\n",
    "print(f\"*** RESULTS: Reader Fine-tuned ***\")\n",
    "print(f'Reader - F1-Score: {metrics[\"Reader\"][\"f1\"]}')\n",
    "print(f'Reader - Exact Match: {metrics[\"Reader\"][\"exact_match\"]}')\n",
    "print(\"******************************************************************\")\n",
    "print(f\"*** All Metrics ***\")\n",
    "print(metrics)\n",
    "print(\"******************************************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72707162-f2bd-4e96-9b3e-0712cadba817",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Base reader evaluation\n",
    "pipeline_qa_base = ExtractiveQAPipeline(reader=reader_base, retriever=retriever_emb)\n",
    "\n",
    "eval_result_pipeline = pipeline_qa_base.eval(\n",
    "            labels=eval_labels,\n",
    "            params={\"Retriever\": {\"top_k\": 10}, \"Reader\": {\"top_k\": 5}}\n",
    "        )\n",
    "\n",
    "# calculate and print metrics\n",
    "metrics = eval_result_pipeline.calculate_metrics()\n",
    "print(f\"*** RESULTS: Reader BASE ***\")\n",
    "print(f'Reader - F1-Score: {metrics[\"Reader\"][\"f1\"]}')\n",
    "print(f'Reader - Exact Match: {metrics[\"Reader\"][\"exact_match\"]}')\n",
    "print(\"******************************************************************\")\n",
    "print(f\"*** All Metrics ***\")\n",
    "print(metrics)\n",
    "print(\"******************************************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbd6bb0-c2db-44e1-93c5-86db028082d2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Large reader evaluation\n",
    "pipeline_qa_large = ExtractiveQAPipeline(reader=reader_large, retriever=retriever_emb)\n",
    "\n",
    "eval_result_pipeline = pipeline_qa_large.eval(\n",
    "            labels=eval_labels,\n",
    "            params={\"Retriever\": {\"top_k\": 10}, \"Reader\": {\"top_k\": 5}}\n",
    "        )\n",
    "\n",
    "# calculate and print metrics\n",
    "metrics = eval_result_pipeline.calculate_metrics()\n",
    "print(f\"*** RESULTS: Reader LARGE ***\")\n",
    "print(f'Reader - F1-Score: {metrics[\"Reader\"][\"f1\"]}')\n",
    "print(f'Reader - Exact Match: {metrics[\"Reader\"][\"exact_match\"]}')\n",
    "print(\"******************************************************************\")\n",
    "print(f\"*** All Metrics ***\")\n",
    "print(metrics)\n",
    "print(\"******************************************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251407d2-e2a2-41d1-8a5d-d6f7611f6fd8",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 4.2.3 QA Pipeline with Ensemble Retriever (BM25 + MFAQ) <a class=\"anchor\" id=\"section_4_2_3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a891064e-5dea-4974-a68e-734d1d2f517e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ensemble Pipeline evaluation\n",
    "from haystack import Pipeline\n",
    "from haystack.pipelines import JoinDocuments\n",
    "\n",
    "pipeline_ensemble = Pipeline()\n",
    "pipeline_ensemble.add_node(component=retriever_bm25, name=\"Retriever_BM25\", inputs=[\"Query\"])\n",
    "pipeline_ensemble.add_node(component=retriever_emb, name=\"Retriever_EMB\", inputs=[\"Query\"])\n",
    "pipeline_ensemble.add_node(component=JoinDocuments(join_mode=\"concatenate\"), name=\"JoinResults\", inputs=[\"Retriever_BM25\", \"Retriever_EMB\"])\n",
    "pipeline_ensemble.add_node(component=reader_fine_tuned, name=\"Reader_Fine_tuned\", inputs=[\"JoinResults\"])\n",
    "\n",
    "\n",
    "eval_result_pipeline = pipeline_ensemble.eval(\n",
    "            labels=eval_labels,\n",
    "            params={\"Retriever_BM25\": {\"top_k\": 10}, \"Retriever_EMB\": {\"top_k\": 10}, \"Reader_Fine_tuned\": {\"top_k\": 5}}\n",
    "            )\n",
    "                    \n",
    "\n",
    "# calculate and print metrics\n",
    "metrics = eval_result_pipeline.calculate_metrics()\n",
    "print(f\"*** RESULTS: Reader LARGE ***\")\n",
    "print(f'Reader - F1-Score: {metrics[\"Reader_Fine_tuned\"][\"f1\"]}')\n",
    "print(f'Reader - Exact Match: {metrics[\"Reader_Fine_tuned\"][\"exact_match\"]}')\n",
    "print(\"******************************************************************\")\n",
    "print(f\"*** All Metrics ***\")\n",
    "print(metrics)\n",
    "print(\"******************************************************************\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qa:Python",
   "language": "python",
   "name": "conda-env-qa-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
